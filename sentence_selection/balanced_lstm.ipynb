{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import queue\n",
    "import random\n",
    "import re\n",
    "from args import get_setup_args\n",
    "import shutil\n",
    "import string\n",
    "#import setup\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import ujson as json\n",
    "#import spacy\n",
    "import json\n",
    "import tensorflow as tf\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do word embeddings instead of sentence embedding for LSTM\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "\n",
    "adversarial_data = None\n",
    "squad_data = None\n",
    "with open('./sentence_selection_data/train_sent_sel_adv.json') as f:\n",
    "    adversarial_data = json.load(f)\n",
    "\n",
    "with open('./sentence_selection_data/train_sent_sel_squad.json') as f:\n",
    "    squad_data = json.load(f)\n",
    "    \n",
    "    vocab = set()\n",
    "    \n",
    "    train_q,train_s,train_val_test_labels = [],[],[]\n",
    "    vocab_max_sent_len = 0\n",
    "    i1,i2 = 0,0\n",
    "for q_id, curr_qa in adversarial_data.items():\n",
    "    if i1 >= 50000:\n",
    "        break\n",
    "    i1+=1\n",
    "    ques = curr_qa['question']\n",
    "    sent = curr_qa['sentence']\n",
    "    answer = curr_qa['contains_answer']\n",
    "    \n",
    "    # Add stuff to vocab\n",
    "    split_ques = ques.split()\n",
    "    split_sent = sent.split()\n",
    "    \n",
    "    vocab.update(split_ques)\n",
    "    vocab.update(split_sent)\n",
    "    \n",
    "    max_len = max(len(split_ques),len(split_sent))\n",
    "\n",
    "    if max_len > vocab_max_sent_len:\n",
    "        vocab_max_sent_len = max_len\n",
    "        \n",
    "\n",
    "    \n",
    "    train_q.append(ques)\n",
    "    train_s.append(sent)\n",
    "    train_val_test_labels.append(answer)\n",
    "    \n",
    "for q_id, curr_qa in squad_data.items():\n",
    "    if i2 >= 50000:\n",
    "        break\n",
    "    i2+=1\n",
    "    ques = curr_qa['question']\n",
    "    sent = curr_qa['sentence']\n",
    "    answer = curr_qa['contains_answer']\n",
    "    \n",
    "    # Add stuff to vocab\n",
    "    split_ques = ques.split()\n",
    "    split_sent = sent.split()\n",
    "    \n",
    "    vocab.update(split_ques)\n",
    "    vocab.update(split_sent)\n",
    "    \n",
    "    max_len = max(len(split_ques),len(split_sent))\n",
    "\n",
    "    if max_len > vocab_max_sent_len:\n",
    "        vocab_max_sent_len = max_len\n",
    "    \n",
    "    train_q.append(ques)\n",
    "    train_s.append(sent)\n",
    "    train_val_test_labels.append(curr_qa['contains_answer'])\n",
    "    \n",
    "vocab_size = len(vocab)\n",
    "    \n",
    "encoded_train_val_test_q = [one_hot(q, vocab_size) for q in train_q]\n",
    "encoded_train_val_test_s = [one_hot(s, vocab_size) for s in train_s]\n",
    "# Train labels don't need to be encoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "#674131 examples? in total \n",
    "# Pad word embeds to be same size\n",
    "encoded_train_val_test_q_padded = sequence.pad_sequences(encoded_train_val_test_q, maxlen=vocab_max_sent_len)\n",
    "encoded_train_val_test_s_padded = sequence.pad_sequences(encoded_train_val_test_s, maxlen=vocab_max_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do 60, 20, 20 split\n",
    "#404479 134826 134826\n",
    "full_sz = len(encoded_train_val_test_q)\n",
    "train_sz = round(full_sz*0.60)\n",
    "val_sz,test_sz = round((full_sz - train_sz) / 2), round((full_sz - train_sz) / 2)\n",
    "encoded_train_q_padded = encoded_train_val_test_q_padded[:train_sz]\n",
    "encoded_val_q_padded = encoded_train_val_test_q_padded[train_sz:train_sz+val_sz]\n",
    "encoded_test_q_padded = encoded_train_val_test_q_padded[train_sz+val_sz:]\n",
    "\n",
    "encoded_train_s_padded = encoded_train_val_test_s_padded[:train_sz]\n",
    "encoded_val_s_padded = encoded_train_val_test_s_padded[train_sz:train_sz+val_sz]\n",
    "encoded_test_s_padded = encoded_train_val_test_s_padded[train_sz+val_sz:]\n",
    "\n",
    "train_labels = train_val_test_labels[:train_sz]\n",
    "val_labels = train_val_test_labels[train_sz:train_sz+val_sz]\n",
    "test_labels = train_val_test_labels[train_sz+val_sz:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adversarial_data = np.load('./sentence_selection_data/train_sent_sel_adv.npz')\n",
    "# squad_data = np.load('./sentence_selection_data/train_sent_sel_squad.npz')\n",
    "# train_q = np.append(adversarial_data['train_q'][:50], squad_data['train_q'][:50], axis = 0)\n",
    "# train_s = np.append(adversarial_data['train_s'][:50], squad_data['train_s'][:50], axis = 0)\n",
    "# train_labels = np.append(adversarial_data['train_labels'][:50], squad_data['train_labels'][:50], axis = 0) \n",
    "\n",
    "# train_q = np.append(adversarial_data['train_q'][:50], squad_data['train_q'][:50], axis = 0)\n",
    "# train_s = np.append(adversarial_data['train_s'][:50], squad_data['train_s'][:50], axis = 0)\n",
    "# train_labels = np.append(adversarial_data['train_labels'][:50], squad_data['train_labels'][:50], axis = 0)\n",
    "\n",
    "#save 100 example debug set\n",
    "# np.savez('./100_ex', train_q=train_q,train_s=train_s,train_labels=train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout, concatenate, LSTM, Dense, concatenate, Embedding\n",
    "embed_len = 32\n",
    "class QUA_Net(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(QUA_Net, self).__init__()\n",
    "        self.q_embed = Embedding(vocab_size, embed_len)\n",
    "        self.s_embed = Embedding(vocab_size, embed_len)\n",
    "        self.q_lstm = LSTM(128)\n",
    "        self.s_lstm = LSTM(128)\n",
    "        self.dropout = Dropout(0.33) #0.3\n",
    "        self.dense = Dense(64, activation='relu')\n",
    "        self.dense2 = Dense(1, activation = 'sigmoid')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        q = self.q_embed(inputs[0])\n",
    "        s = self.s_embed(inputs[1])\n",
    "        \n",
    "#         exit()\n",
    "        q_out = self.q_lstm(q)\n",
    "\n",
    "        s_out = self.s_lstm(s)\n",
    "        \n",
    "        merge = concatenate([q_out, s_out], axis = -1)\n",
    "        drop = self.dropout(merge)\n",
    "        drop2 = self.dense(drop)\n",
    "        out = self.dense2(drop2)\n",
    "        return out\n",
    "\n",
    "model = QUA_Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "opt = optimizers.Adam(lr=1e-2) #1e-3\n",
    "\n",
    "# What optimizer should we use/ metrics?\n",
    "model.compile(optimizer=opt,loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos = np.where(train_labels == True)\n",
    "# print(len(pos[0]))\n",
    "# pos_idxs = pos[0]\n",
    "\n",
    "# negs = np.where(train_labels == False)\n",
    "# print(len(negs[0]))\n",
    "\n",
    "# neg_idxs = np.random.choice(negs[0], len(pos[0]))\n",
    "# print(len(neg_idxs), neg_idxs)\n",
    "# print(train_q[0].shape)\n",
    "# train_balanced_q = np.take(train_q, np.append(pos_idxs, neg_idxs), axis=0)\n",
    "# train_balanced_s = np.take(train_s, np.append(pos_idxs, neg_idxs), axis=0)\n",
    "# train_balanced_labels = np.take(train_labels, np.append(pos_idxs, neg_idxs), axis=0)\n",
    "# print(train_balanced_q.shape)\n",
    "# print(len(train_balanced_q), len(train_balanced_s), len(train_balanced_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37435,)\n",
      "(37435, 161)\n",
      "(37435, 161)\n",
      "(12478, 161)\n",
      "(12478, 161)\n",
      "Train on 37435 samples, validate on 12478 samples\n",
      "Epoch 1/20\n",
      "37435/37435 [==============================] - 159s 4ms/sample - loss: 0.4865 - acc: 0.7927 - val_loss: 0.5956 - val_acc: 0.7625\n",
      "Epoch 2/20\n",
      "37435/37435 [==============================] - 155s 4ms/sample - loss: 0.4035 - acc: 0.8297 - val_loss: 0.5846 - val_acc: 0.7363\n",
      "Epoch 3/20\n",
      "37435/37435 [==============================] - 158s 4ms/sample - loss: 0.3509 - acc: 0.8470 - val_loss: 0.6206 - val_acc: 0.7325\n",
      "Epoch 4/20\n",
      " 9280/37435 [======>.......................] - ETA: 1:52 - loss: 0.3107 - acc: 0.8629"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "\n",
    "\n",
    "EPOCHS = 20\n",
    "train_labels = np.asarray(train_labels)\n",
    "test_labels = np.asarray(test_labels)\n",
    "val_labels = np.asarray(val_labels)\n",
    "\n",
    "print(train_labels.shape)\n",
    "print(encoded_train_q_padded.shape) #first dim is num examples, 2nd dim is longest string in qa dataset\n",
    "print(encoded_train_s_padded.shape)\n",
    "\n",
    "print(encoded_val_q_padded.shape)\n",
    "print(encoded_val_s_padded.shape)\n",
    "\n",
    "\n",
    "\n",
    "true_amount = len(train_labels[train_labels == True]) / len(train_val_test_labels)\n",
    "false_amount = 1. - true_amount\n",
    "\n",
    "assert(false_amount+true_amount == 1.0)\n",
    "\n",
    "class_imb_dict = {False:false_amount, True:true_amount}\n",
    "\n",
    "#weights = class_weight.compute_class_weight('balanced', np.unique(train_labels[:1000]), train_labels[:1000])\n",
    "# hist = model.fit(x=[encoded_train_q_padded,encoded_train_s_padded],y=train_labels,validation_data=([encoded_val_q_padded,encoded_val_s_padded], val_labels),epochs=EPOCHS,batch_size=1, class_weight=class_imb_dict)\n",
    "# hist = model.fit(x=[train_balanced_q[:,:,np.newaxis], train_balanced_s[:,:,np.newaxis]], y = train_balanced_labels, epochs = EPOCHS, batch_size=32, callbacks=callbacks_list)\n",
    "hist = model.fit(x=[encoded_train_q_padded,encoded_train_s_padded], y=train_labels,validation_data=([encoded_val_q_padded,encoded_val_s_padded],val_labels),epochs=EPOCHS,batch_size=64)\n",
    "# ADD CLAS IMB DICT LATER ONCE WE KNOW IT\"LL WORK LOL APPARENTLY IT ONLY HELPS A LITLLE BIT\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# y = model.predict(x=[train_balanced_q[:,:,np.newaxis], train_balanced_s[:,:,np.newaxis]])\n",
    "# print(encoded_test_q_padded[0])\n",
    "# print(encoded_test_s_padded[0])\n",
    "\n",
    "# Get indexes corr. to true positives\n",
    "# correct_test_labels_idxs = np.where(test_labels == True)\n",
    "# print(correct_test_labels_idxs)\n",
    "\n",
    "# predictions = model.predict(x=[ [encoded_test_q_padded[163]],[encoded_test_s_padded[163]] ])\n",
    "predictions = model.predict(x=[encoded_test_q_padded,encoded_test_s_padded])\n",
    "\n",
    "print(predictions)\n",
    "predictions = [True if val >= 0.5 else False for val in predictions]\n",
    "cm = confusion_matrix(test_labels, predictions)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
