{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    }
   ],
   "source": [
    "# Sentence encodings\n",
    "from bert_sent_encoding import bert_sent_encoding\n",
    "\n",
    "\n",
    "# Import pre-trained Bert for sentence embeds\n",
    "bse = bert_sent_encoding(model_path='bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import queue\n",
    "import random\n",
    "import re\n",
    "from args import get_setup_args\n",
    "import shutil\n",
    "import string\n",
    "#import setup\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from collections import Counter\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import ujson as json\n",
    "#import spacy\n",
    "import json\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_question_contexts():\n",
    "# Read in train test, dev jsons\n",
    "# Get each paragraph\n",
    "\n",
    "# TRAIN\n",
    "# Split into {question_id -> (\"question_string\", [\"sent1_str\", \"sent2_str\", \"sent3_str\", ...]))}\n",
    "# Split into {qid -> (q_embed,[sent1_embed, sent2_embed, sent3_embed, ....])}\n",
    "\n",
    "#DEV + TEST\n",
    "# Do same for dev + test\n",
    "\n",
    "#populate three dictionaries\n",
    "\n",
    "    datasets = [\"dev\",\"test\",\"train\"]\n",
    "    dev_data = train_data = test_data = None\n",
    "    v_str = \"-v2.0.json\"\n",
    "\n",
    "    for ds in datasets:\n",
    "        filename = ds + v_str\n",
    "        with open(filename) as f:\n",
    "            json_file = json.load(f)\n",
    "\n",
    "                \n",
    "\n",
    "            # outermost is a keys list of [version, data]\n",
    "            data = json_file[\"data\"]\n",
    "            # Data is a list of dicts\n",
    "            for entry in data:\n",
    "            \n",
    "                # Each entry is a dict of keys [title, paragraphs]\n",
    "#                 print(entry['title']) #Title example is normans\n",
    "                #Paragraphs is a list of dicts\n",
    "    #            print(ex_entry['paragraphs'])\n",
    "\n",
    "\n",
    "                paragraphs = entry['paragraphs']\n",
    "    #             print(ex_paragraphs[0])\n",
    "            \n",
    "                # Want to create new paragraphs list \n",
    "                context_to_qa_list = [] # REPLACE paragraphs\n",
    "                # Each paragraph has a list of questions and associated answers and context\n",
    "                # each paragraph is a dict with keys [qas, context]  \n",
    "                #ex_paragraph['context'] is just sent paragraph\n",
    "    #             print(ex_paragraph['qas'])\n",
    "                for paragraph in paragraphs:\n",
    "                    context = paragraph['context']   \n",
    "                    for qa in paragraph['qas']:\n",
    "                        # Append to 1:1 dict list here\n",
    "                        context_to_qa_list.append({'qas':qa,'context':context})\n",
    "                    \n",
    "                entry['paragraphs'] = context_to_qa_list\n",
    "                \n",
    "                if ds == \"dev\":\n",
    "                    dev_data = json_file\n",
    "#                     print(dev_data['data'][0]['paragraphs'][0])\n",
    "                elif ds == \"test\":\n",
    "                    test_data = json_file\n",
    "                else:\n",
    "                    train_data = json_file\n",
    "            \n",
    "    return dev_data,train_data,test_data\n",
    "\n",
    "#             vectors = bse.get_vector(['sent2', 'sent3'], word_vector=False, layer=-1)\n",
    "#             print(vectors)\n",
    "#             vectors = bse.get_vector(['sent2', 'sent3'], word_vector=False, layer=-1)  # 4th line 2. get vector list of strings\n",
    "#            bse.write_txt2vector(input_file, output_file, word_vector=False, layer=-1)   # 5th line 3. get and write vectors of strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps all dev,train, and test to 1:1 in regards to question and context\n",
    "# no longer have multiple questions per context, now ea. q-a gets its own context :3\n",
    "data = pair_question_contexts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from scipy.spatial.distance import cosine as cos_sim\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR NOW COSINE SIM, but NEXT STEP IS REPLACING COS_SIM WITH MODEL PREDICTION\n",
    "def remove_least_relevant_sents(data):\n",
    "    for dataset in data:\n",
    "        for entry in dataset['data']:\n",
    "            paragraphs = entry['paragraphs']\n",
    "            for paragraph in paragraphs:\n",
    "                punct = '!.?'\n",
    "                curr_context = paragraph['context']\n",
    "                curr_qa = paragraph['qas']\n",
    "                question = curr_qa['question']\n",
    "                q_embed = bse.get_vector(question, word_vector=False, layer=-1)\n",
    "                \n",
    "                # THIS IS IN CASE OF PUNCTUATION INSIDE OF QUOTES(for dialogue) NOT FINISHED \n",
    "    #             context_as_sentences = None\n",
    "    #             if '\"' in curr_context or \"'\" in curr_context:\n",
    "    #                 lst = []\n",
    "    #                 prev_index = 0\n",
    "    #                 has_closed = True\n",
    "    #                 for i in range(curr_context):\n",
    "    #                     if curr_context[i] == \"\"\"\"\"\" or curr_context[i] == \"''\":\n",
    "    #                         has_closed = not has_closed\n",
    "    #                     if curr_context[i] in punct:\n",
    "    #                         context_as_sentences.append(context[prev_index: i])\n",
    "    #                 context_as_sentences = lst\n",
    "                \n",
    "                # Split up context into sentences\n",
    "                context_as_sentences = re.split(r\"([!|.|?])+\",curr_context)\n",
    "                context_as_sentences, punctuation = context_as_sentences[0::2], context_as_sentences[1::2]\n",
    "                \n",
    "\n",
    "                \n",
    "                context_as_sentences = list(filter(lambda x: x != \"\",context_as_sentences))\n",
    "#                 print(context_as_sentences)\n",
    "                # Get context embeddings\n",
    "                context_embeds = bse.get_vector(context_as_sentences, word_vector=False, layer=-1)\n",
    "                \n",
    "                #Note that spatial.distance.cosine computes the distance, and not the similarity. \n",
    "                #So, you must subtract the value from 1 to get the similarity\n",
    "\n",
    "                lowest_sent_sim = 1.1 \n",
    "                sent_index_to_remove = -1\n",
    "                # Find worst sentence\n",
    "                for index,sent_embed in enumerate(context_embeds):\n",
    "                    embed_similarity = 1 - cos_sim(q_embed, sent_embed)\n",
    "                    if embed_similarity < lowest_sent_sim:\n",
    "                        sent_index_to_remove = index\n",
    "                        \n",
    "                # Remove least relevant sentence\n",
    "                del context_as_sentences[sent_index_to_remove]\n",
    "                del punctuation[sent_index_to_remove]\n",
    "                \n",
    "                new_context = str([sent + punc] for (sent,punc) in zip(context_as_sentences,punctuation))\n",
    "#                 print(new_context)\n",
    " \n",
    "                return\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France', ' They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia', ' Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia', ' The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries']\n"
     ]
    }
   ],
   "source": [
    "remove_least_relevant_sents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
